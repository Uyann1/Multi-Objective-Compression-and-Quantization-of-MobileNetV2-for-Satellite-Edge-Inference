{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq9jKt_qGdRQ"
   },
   "source": [
    "# Section 1: Environment & Hardware Setup\n",
    "**Objective:** Initialize the deep learning stack and secure a persistent, accelerated computing environment.\n",
    "\n",
    "**Software Stack:** We import tensorflow to build the differentiable computation graph and numpy for high-performance tensor operations. Utilities like json and os are included for structured logging and file management.\n",
    "\n",
    "**Persistence Layer:** We mount Google Drive to establish a permanent save_path. This allows us to serialize model checkpoints and training history in real-time, mitigating the risk of data loss from Colab's ephemeral runtime.\n",
    "\n",
    "**Hardware Acceleration:** The script explicitly validates the availability of a CUDA-enabled GPU (e.g., NVIDIA T4). GPU parallelism is strictly required to accelerate the heavy matrix multiplications inherent in training CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json, numpy as np, tensorflow as tf\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "save_path = \"/content/drive/MyDrive/Satellite_Project_LAB\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print(\"GPU NOT FOUND. Go to Edit > Notebook Settings and select T4 GPU.\")\n",
    "else:\n",
    "    print(f\"Success! Found GPU at: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bza-y-RZRgvC"
   },
   "source": [
    "# Section 2: Data Acquisition & Preprocessing Pipeline\n",
    "**Objective:** Load the EuroSAT dataset and build an optimized tf.data pipeline for efficient model training.\n",
    "\n",
    "**Dataset Selection:** We utilize the EuroSAT (RGB) dataset, consisting of 27,000 satellite images across 10 Land Use and Land Cover (LULC) classes. The data is partitioned into 80/10/10 splits for training, validation, and testing.\n",
    "\n",
    "**Image Standardization:**  All images are resized to 224x224 pixels to match the input requirements of the MobileNetV2 architecture. We apply preprocess_input to scale pixel values to the [-1, 1] range, ensuring numerical stability during gradient descent.\n",
    "\n",
    "**Data Augmentation:** To improve model generalization and mitigate overfitting, we apply stochastic geometric transformations—including horizontal and vertical flips—to the training set.\n",
    "\n",
    "**Pipeline Optimization:**\n",
    "\n",
    "* Vectorized Mapping: Preprocessing and augmentation are applied via .map() for efficient element-wise execution.\n",
    "\n",
    "* Prefetching: We use tf.data.AUTOTUNE to overlap data preprocessing with model execution, eliminating CPU bottlenecks.\n",
    "\n",
    "* Batching & Shuffling: Data is shuffled to maintain i.i.d. (independent and identically distributed) properties and batched to optimize throughput on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    'eurosat/rgb',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(preprocess).map(augment).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = val_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = test_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Dataset loaded. Classes: {info.features['label'].names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_names = info.features['label'].names\n",
    "\n",
    "# Load a raw (un-preprocessed) train split just for visualization\n",
    "raw_train = tfds.load('eurosat/rgb', split='train', shuffle_files=False, as_supervised=True)\n",
    "\n",
    "examples = {}\n",
    "for img, lbl in raw_train:\n",
    "    lbl_int = int(lbl.numpy())\n",
    "    if lbl_int not in examples:\n",
    "        examples[lbl_int] = img.numpy()\n",
    "    if len(examples) == len(class_names):\n",
    "        break\n",
    "\n",
    "n = len(class_names)\n",
    "cols = 5\n",
    "rows = (n + cols - 1) // cols\n",
    "plt.figure(figsize=(3*cols, 3*rows))\n",
    "for i, (lbl_int, img_arr) in enumerate(sorted(examples.items())):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(img_arr)\n",
    "    plt.title(class_names[lbl_int], fontsize=9)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uXuFo0FRlsQ"
   },
   "source": [
    "# Section 3: Model Architecture & Robust Training Strategy\n",
    "![Model Architecture](imgs/Gemini_Generated_Image_vo7kijvo7kijvo7k%20(1).png)\n",
    "## Objective: To develop a family of lightweight models using Transfer Learning and to implement a rigorous statistical framework for performance verification.\n",
    "### 1. Architectural Foundation: MobileNetV2\n",
    "\n",
    "For satellite imagery deployment, we require a model that balances high accuracy with a low computational footprint. MobileNetV2 is an optimal choice because it utilizes Inverted Residual Blocks and Depthwise Separable Convolutions. These features significantly reduce the number of parameters and Multiply-Accumulate (MAC) operations compared to standard CNNs, making the architecture highly suitable for resource-constrained edge devices.\n",
    "\n",
    "Detailed information on these features can be found in the original research paper: [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/pdf/1801.04381).\n",
    "\n",
    "**1. Depthwise Separable Convolutions**\n",
    "![Model Architecture](imgs/conv.png)\n",
    "\n",
    "The core efficiency of MobileNetV2 comes from replacing standard convolutions with Depthwise Separable Convolutions. This process factorizes a traditional convolution into two distinct stages:\n",
    "\n",
    "* **Depthwise Convolution:** Instead of applying a single filter across all input channels, a separate $k \\times k$ filter is applied to each input channel independently.\n",
    "* **Pointwise Convolution ($1 \\times 1$):** A $1 \\times 1$ convolution is then used to combine the outputs of the depthwise stage into a new set of output channels.\n",
    "\n",
    "#### Mathematical Parameter Reduction\n",
    "Let $h \\times w$ be the feature map size, $d_i$ the number of input channels, $d_o$ the output channels, and $k \\times k$ the kernel size.\n",
    "\n",
    "**Standard Convolution Cost:** $$h \\cdot w \\cdot d_i \\cdot d_o \\cdot k^2$$\n",
    "\n",
    "**Depthwise Separable Cost:** $$(h \\cdot w \\cdot d_i \\cdot k^2) + (h \\cdot w \\cdot d_i \\cdot d_o)$$\n",
    "\n",
    "The Reduction Ratio is calculated as:\n",
    "$$\\frac{\\text{Separable Cost}}{\\text{Standard Cost}} = \\frac{h \\cdot w \\cdot d_i (k^2 + d_o)}{h \\cdot w \\cdot d_i \\cdot d_o \\cdot k^2} = \\frac{1}{d_o} + \\frac{1}{k^2}$$\n",
    "\n",
    "For a standard $3 \\times 3$ convolution, this results in nearly an 8x to 9x reduction in computational cost and parameters with only a minimal trade-off in accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Inverted Residual Blocks & Linear Bottlenecks**\n",
    "![Model Architecture](imgs/residuals.png)\n",
    "\n",
    "In traditional residual blocks (e.g., ResNet), the structure follows a Wide $\\rightarrow$ Narrow $\\rightarrow$ Wide pattern regarding channel depth. MobileNetV2 flips this convention, using an Inverted Residual structure: Narrow $\\rightarrow$ Wide $\\rightarrow$ Narrow.\n",
    "\n",
    "#### The Three-Step Block Structure\n",
    "* **Expansion Layer ($1 \\times 1$):** This layer takes the narrow input and expands it to a higher dimension (controlled by an expansion factor) to allow the model to learn more complex features in a higher-dimensional space.\n",
    "* **Depthwise Convolution ($3 \\times 3$):** This stage performs the actual spatial filtering in that high-dimensional space efficiently.\n",
    "* **Projection Layer / Linear Bottleneck ($1 \\times 1$):** This final stage projects the high-dimensional features back down to a narrow \"bottleneck\".\n",
    "\n",
    "#### Why the \"Linear\" Bottleneck?\n",
    "The researchers found that applying non-linear activation functions (like ReLU) in narrow layers causes significant information loss. To prevent this, the last $1 \\times 1$ layer in the block uses a linear activation, preserving the manifold of interest. By using a narrow \"bottleneck\" for the skip connections, the model drastically reduces the number of parameters required to store the residuals while maintaining high-precision processing.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Implementation Summary**\n",
    "* **Transfer Learning & Frozen Layers:** We utilize a pre-trained backbone on ImageNet to leverage low-level feature detectors (edges, textures). The base model is frozen (trainable = False), while we train a custom Classification Head consisting of Global Average Pooling, a 20% Dropout layer for regularization, and a 10-class Softmax layer.\n",
    "\n",
    "\n",
    "---\n",
    "### 2. The Alpha ($\\alpha$) Parameter: Width Scaling\n",
    "\n",
    "We train models at three different \"Width Multipliers\" ($\\alpha$): 1.0, 0.5, and 0.35.\n",
    "\n",
    "**Reasoning:** The $\\alpha$ parameter uniformly thins the number of filters in each layer. This allows us to explore the Pareto frontier between Model Size and Classification Accuracy, which is critical for choosing the right model for specific satellite hardware constraints.\n",
    "\n",
    "### 3. Statistical Rigor: Multi-Seed Training\n",
    "\n",
    "We train each configuration across three different random seeds (1, 2, 3). This practice is conceptually motivated by the Law of Large Numbers (LLN)(https://en.wikipedia.org/wiki/Law_of_large_numbers). While this sample size is not large enough, due to computational constraints, to strictly satisfy the conditions of the LLN, it still provides a more reliable performance estimate than a single run (n=1).\n",
    "\n",
    "**The Mathematical Reasoning:** Deep Learning training is a stochastic process (random weight initialization, data shuffling). A single training run is a single sample from a distribution of possible outcomes. To report the true performance of an architecture, we need to estimate the Population Mean ($\\mu$).\n",
    "\n",
    "**Law of Large Numbers (LLN) Context:** As the number of experiments ($n$) increases, the sample mean $\\bar{X}_n$ converges almost surely to the expected value $\\mu$.$$\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\xrightarrow{n \\to \\infty} \\mu$$By using multiple seeds, we can calculate the Mean $\\pm$ Standard Deviation, ensuring our results are not just \"lucky\" outliers but statistically significant and reproducible.\n",
    "\n",
    "**4. Experiment Tracking: JSON History Serialization**\n",
    "\n",
    "We save all training and validation metrics (Accuracy/Loss) into a JSON-safe format. Why JSON? Unlike binary formats, JSON is a human-readable, language-independent standard.\n",
    "\n",
    "**Reasoning:**  This allows us to perform \"Post-Mortem\" analysis and visualization without needing to re-run the computationally expensive training process. We cast values to float to ensure compatibility with standard web-based visualization tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 — Train FP32 models (multi-seed, multi-alpha) and save histories (JSON-safe)\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "seeds = [1, 2, 3]\n",
    "alphas = [1.0, 0.5, 0.35]\n",
    "epochs = 15\n",
    "\n",
    "histories = {a: [] for a in alphas}   # list of history dicts per alpha\n",
    "fp32_paths = {a: [] for a in alphas}  # model paths per alpha/seed\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def build_model(alpha):\n",
    "    base = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        alpha=alpha\n",
    "    )\n",
    "    base.trainable = False\n",
    "    m = models.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return m\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"\\n=== Training alpha={alpha} for seeds {seeds} ===\")\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)\n",
    "        model = build_model(alpha)\n",
    "        print(f\"\\nModel (alpha={alpha}, seed={seed})\")\n",
    "        model.summary()\n",
    "\n",
    "        hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=1)\n",
    "        histories[alpha].append(hist.history)\n",
    "\n",
    "        fp32_file = f\"{save_path}/satellite_model_alpha{alpha}_fp32_seed{seed}.h5\"\n",
    "        model.save(fp32_file)\n",
    "        fp32_paths[alpha].append(fp32_file)\n",
    "        print(f\"Saved: {fp32_file}\")\n",
    "\n",
    "# Convert histories to JSON-safe (cast all values to float) and save\n",
    "histories_json_safe = {}\n",
    "for alpha in histories:\n",
    "    runs = []\n",
    "    for h in histories[alpha]:\n",
    "        runs.append({k: [float(x) for x in v] for k, v in h.items()})\n",
    "    histories_json_safe[alpha] = runs\n",
    "\n",
    "with open(f\"{save_path}/histories.json\", \"w\") as f:\n",
    "    json.dump(histories_json_safe, f)\n",
    "print(\"Saved histories.json (JSON-safe)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6dWcO-ap_Rn"
   },
   "source": [
    "# Section 4: INT8 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3mjxup_LOPo"
   },
   "source": [
    "**Model State Recovery & Path Mapping Objective:** To verify the persistence of the trained FP32 models and re-establish the file-system mapping required for the Post-Training Quantization (PTQ) phase.\n",
    "\n",
    "**The Engineering Rationale**\n",
    "\n",
    "Deep learning workflows often require separating the Training and Optimization stages due to resource management, session timeouts, or hardware shifts. This section acts as a bridge:\n",
    "\n",
    "**Session Decoupling:** By explicitly searching for existing .h5 model files in the save_path, we decouple the training process from the optimization process. This allows the project to remain modular and resilient to environment restarts.\n",
    "\n",
    "**Validation of Persistence:** The script iterates through every combination of Width Multiplier ($\\alpha$) and Random Seed, confirming that each of the 9 experimental models was successfully serialized to Google Drive.\n",
    "\n",
    "**Dynamic Indexing:** Instead of hardcoding file names, we programmatically build the fp32_paths dictionary. This ensures that the subsequent TFLite conversion scripts have a verified, error-free list of source models to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to tell the notebook where your already-trained models are\n",
    "import os\n",
    "alphas = [1.0, 0.5, 0.35]\n",
    "seeds = [1, 2, 3]\n",
    "fp32_paths = {a: [] for a in alphas}\n",
    "\n",
    "for a in alphas:\n",
    "    for s in seeds:\n",
    "        path = f\"{save_path}/satellite_model_alpha{a}_fp32_seed{s}.h5\"\n",
    "        if os.path.exists(path):\n",
    "            fp32_paths[a].append(path)\n",
    "            print(f\"Found: {path}\")\n",
    "        else:\n",
    "            print(f\"MISSING: {path} - Check your Drive folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAvdh46UQPcc"
   },
   "source": [
    "# Section 4 — Post-Training INT8 Quantization & Calibration\n",
    "\n",
    "**Objective:** To compress the high-precision FP32 models into an 8-bit integer format (INT8), enabling deployment on specialized edge hardware and satellite edge processors / flight hardware with significant performance gains.\n",
    "\n",
    "### 1. Why INT8?\n",
    "\n",
    "While our initial models use 32-bit Floating Point (FP32) numbers for training, most edge-computing hardware (like those found in satellite payloads or mobile devices) is optimized for Integer arithmetic. INT8 Quantization is the process of mapping floating-point weights and activations $x \\in \\mathbb{R}$ into a discrete range of 256 integers $q \\in \\mathbb{Z}$ (from $-128$ to $127$).\n",
    "\n",
    "### 2. The Calibration Process: Representative Datasets\n",
    "\n",
    "Converting to INT8 without data-driven context typically results in significant accuracy degradation. To mitigate this, we implement **Post-training static quantization** via a representative dataset.\n",
    "\n",
    "**The Problem:** We must determine the dynamic range $[min_x, max_x]$ of activations in each layer to map them to the 8-bit space without excessive clipping.\n",
    "\n",
    "**The Math:** To perform this mapping, the converter must calculate two parameters for every layer: the **Scale ($S$)** and the **Zero-point ($Z$)**:\n",
    "\n",
    "1. **Scale ($S$):** Determines the step size of the quantizer.\n",
    "   $$S = \\frac{max_{observed} - min_{observed}}{q_{max} - q_{min}}$$\n",
    "   *where $q_{max} = 127$ and $q_{min} = -128$ for signed INT8.*\n",
    "\n",
    "2. **Zero-point ($Z$):** Ensures that the floating-point value $0.0$ is mapped exactly to an integer.\n",
    "   $$Z = q_{min} - \\text{round}\\left( \\frac{min_{observed}}{S} \\right)$$\n",
    "\n",
    "**Our Solution:** We built a `representative_cache` by sampling 120 images per class from the validation set ($1,200$ images total).\n",
    "\n",
    "**The Benefit:** This allows the converter to \"calibrate\" these $S$ and $Z$ values for each layer, theoretically minimizing the quantization noise introduced by the precision reduction.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Comparative Analysis: Theoretical Trade-offs\n",
    "\n",
    "| Feature | FP32 (Base) | INT8 (Quantized) | Projected Engineering Impact |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Storage Size** | ~9.5MB (Alpha 1.0) | **~2.7MB** | **Theoretical ~4x reduction**; critical for L1/L2 cache residency. |\n",
    "| **Inference Speed** | Baseline | **Accelerated** | Potential for massive throughput gains on integer-only hardware. |\n",
    "| **Power Profile** | High | **Low** | Reduced bit-width translates directly to lower orbital power draw. |\n",
    "| **Accuracy** | Reference | **Stochastic Drop** | Expected variance in precision; sensitive to model width ($\\alpha$). |\n",
    "\n",
    "### 4. Technical Implementation Highlights\n",
    "\n",
    "We utilize the `TFLiteConverter` to transform the FP32 weights into quantized values using the transformation:\n",
    "\n",
    "$$q = \\text{clamp}\\left( \\text{round}\\left( \\frac{x}{S} + Z \\right), q_{min}, q_{max} \\right)$$\n",
    "\n",
    "We apply strict constraints to ensure full hardware compatibility:\n",
    "\n",
    "* **`Optimize.DEFAULT`**: A heuristic-driven approach to balance latency and size.\n",
    "* **`supported_ops = [TFLITE_BUILTINS_INT8]`**: Enforces a strict integer-only graph, removing reliance on Floating Point Units (FPU).\n",
    "* **`inference_input_type = tf.int8`**: Optimizes the \"first mile\" of inference by accepting integer tensors directly, avoiding CPU-bound float-to-int casting at the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "CAL_SAMPLES_PER_CLASS = 120  # adjust 80–150 if you need faster/slower\n",
    "\n",
    "def build_representative_cache(dataset, per_class=CAL_SAMPLES_PER_CLASS, num_classes=NUM_CLASSES):\n",
    "    counts = [0] * num_classes\n",
    "    cache = []\n",
    "    for data, label in dataset.unbatch():  # val_ds is preprocessed, non-augmented\n",
    "        c = int(label.numpy())\n",
    "        if counts[c] >= per_class:\n",
    "            continue\n",
    "        cache.append(data.numpy())\n",
    "        counts[c] += 1\n",
    "        if all(x >= per_class for x in counts):\n",
    "            break\n",
    "    return cache\n",
    "\n",
    "rep_cache = build_representative_cache(val_ds)\n",
    "print(f\"Built representative cache with {len(rep_cache)} samples \"\n",
    "      f\"({CAL_SAMPLES_PER_CLASS} per class x {NUM_CLASSES} classes)\")\n",
    "\n",
    "def representative_dataset():\n",
    "    for arr in rep_cache:\n",
    "        yield [np.expand_dims(arr, axis=0).astype(np.float32)]\n",
    "\n",
    "int8_paths = {a: [] for a in alphas}\n",
    "\n",
    "for alpha in alphas:\n",
    "    for fp32_file in fp32_paths[alpha]:\n",
    "        model = tf.keras.models.load_model(fp32_file)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "        print(f\"Quantizing to INT8: {fp32_file}\")\n",
    "        tflite_int8 = converter.convert()\n",
    "        int8_file = fp32_file.replace(\"_fp32_\", \"_int8_\").replace(\".h5\", \".tflite\")\n",
    "        with open(int8_file, \"wb\") as f:\n",
    "            f.write(tflite_int8)\n",
    "        int8_paths[alpha].append(int8_file)\n",
    "        print(f\"Saved: {int8_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ1Bk1P6y3Ip"
   },
   "source": [
    "# Section 5: Post-Training Float16 Quantization\n",
    "**Objective**\n",
    "\n",
    "To implement Post-Training Quantization (PTQ) by converting high-precision FP32 models into reduced-precision FP16 format. This stage aims to optimize the models for deployment by significantly reducing latency and storage requirements while preserving classification accuracy.\n",
    "\n",
    "**1. Transitioning from FP32 to FP16**\n",
    "Standard deep learning models are trained using 32-bit Floating Point (FP32) precision. However, for inference on edge devices, this level of precision often introduces unnecessary computational overhead. By transitioning to 16-bit Floating Point (FP16):\n",
    "\n",
    "**Bit-Width Reduction:** We reduce the memory footprint of each weight and activation by 50% (from 32 bits to 16 bits).\n",
    "\n",
    "**Dynamic Range Preservation:** Unlike INT8 quantization, FP16 maintains a floating-point representation (using a 5-bit exponent and 10-bit mantissa). This allows the model to represent a wide range of values, which is critical for maintaining the subtle spectral features present in EuroSAT satellite imagery.\n",
    "\n",
    "**2. Comparative Analysis: Observed Performance Metrics**\n",
    "\n",
    "| Metric | FP32 (Base) | FP16 (Optimized) | Engineering Impact |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Storage Size** | 100% (~9.55MB) | **~42-47% (~4.49MB)** | **2.1x to 2.5x reduction**; optimizes on-device flash memory residency. |\n",
    "| **Inference Speed** | Baseline | **Significant Increase** | **9.8x to 23.4x throughput increase** on compatible hardware. |\n",
    "| **Accuracy Loss** | Reference | **Near-Lossless** | **Negligible drop (0.07 to 0.27 pts)**; preserves model reliability. |\n",
    "| **Numeric Range** | High | **Sufficient** | Maintains sufficient dynamic range for complex LULC classification. |\n",
    "\n",
    "**3. Advantages and Disadvantages**\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "**Significant Throughput Increase:** By reducing the bit-width, we effectively reduce memory traffic by ~2×. On hardware with FP16 acceleration (such as modern GPUs and specialized NPUs), this results in a substantial reduction in inference time (e.g., reducing latency from ~118ms to ~5ms for $\\alpha=0.35$).\n",
    "\n",
    "**Minimal Accuracy Degradation:** Due to the preservation of the floating-point structure, the \"quantization noise\" is extremely low. Our results show that the model retains its predictive power across all EuroSAT classes with less than a 0.3% accuracy trade-off.\n",
    "\n",
    "**Memory Efficiency:** Reducing the model size allows for more efficient caching and lower power consumption during data movement within the hardware architecture.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "**Hardware Requirements:** Optimal speedups require hardware that supports FP16 arithmetic (Half-Precision). On older CPUs without dedicated FP16 support, the model may run at FP32 speeds due to software-level emulation.\n",
    "\n",
    "**4. Technical Implementation Details**\n",
    "\n",
    "The conversion is performed using the TFLiteConverter with the following configuration:\n",
    "\n",
    "tf.lite.Optimize.DEFAULT: Enables a suite of optimizations for size and latency.\n",
    "\n",
    "target_spec.supported_types = [tf.float16]: Explicitly instructs the compiler to cast the 32-bit weights into 16-bit floating-point values.\n",
    "\n",
    "Unlike INT8 quantization, FP16 conversion does not require a representative dataset because floating-point arithmetic is preserved, eliminating the need for calibration. This makes the optimization pipeline more robust and less prone to the \"feature collapse\" sometimes observed in integer-only quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_paths = {a: [] for a in alphas}\n",
    "\n",
    "for alpha in alphas:\n",
    "    for fp32_file in fp32_paths[alpha]:\n",
    "        model = tf.keras.models.load_model(fp32_file)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "        print(f\"Quantizing to FP16: {fp32_file}\")\n",
    "        tflite_fp16 = converter.convert()\n",
    "        fp16_file = fp32_file.replace(\"_fp32_\", \"_fp16_\").replace(\".h5\", \".tflite\")\n",
    "        with open(fp16_file, \"wb\") as f:\n",
    "            f.write(tflite_fp16)\n",
    "        fp16_paths[alpha].append(fp16_file)\n",
    "        print(f\"Saved: {fp16_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHjKS9gNza9W"
   },
   "source": [
    "# Section 6: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diCnlmPk_OnZ"
   },
   "source": [
    "## Optimized Asset Verification & Path Mapping\n",
    "\n",
    "**Objective:** To verify the persistent storage of all optimized INT8 and FP16 TFLite models and re-establish file-system indexing required for the final multi-metric evaluation.\n",
    "\n",
    "**The Engineering Rationale**\n",
    "\n",
    "**Quantized Asset Recovery:** This step ensures that the 18 optimized variants (3 alphas × 3 seeds × 2 formats) generated in the previous phase are correctly serialized and accessible, preventing session-loss errors.\n",
    "\n",
    "**Pipeline Decoupling:** By re-indexing the assets at the start of Section 6, the evaluation remains independent of the quantization process, allowing for modular testing and easier data recovery.\n",
    "\n",
    "**Automated Indexing:** The script programmatically populates the int8_paths and fp16_paths dictionaries, ensuring the subsequent evaluation loops can automatically iterate through every model variant without manual path entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure these lists are ready\n",
    "alphas = [1.0, 0.5, 0.35]\n",
    "seeds = [1, 2, 3]\n",
    "\n",
    "# Initialize the path dictionaries\n",
    "int8_paths = {a: [] for a in alphas}\n",
    "fp16_paths = {a: [] for a in alphas}\n",
    "\n",
    "print(\"--- Searching for saved models in Drive ---\")\n",
    "\n",
    "for a in alphas:\n",
    "    for s in seeds:\n",
    "        # 1. Check for INT8 (.tflite)\n",
    "        int8_file = f\"{save_path}/satellite_model_alpha{a}_int8_seed{s}.tflite\"\n",
    "        if os.path.exists(int8_file):\n",
    "            int8_paths[a].append(int8_file)\n",
    "        else:\n",
    "            print(f\"MISSING INT8: alpha {a} seed {s}\")\n",
    "\n",
    "        # 2. Check for FP16 (.tflite)\n",
    "        fp16_file = f\"{save_path}/satellite_model_alpha{a}_fp16_seed{s}.tflite\"\n",
    "        if os.path.exists(fp16_file):\n",
    "            fp16_paths[a].append(fp16_file)\n",
    "        else:\n",
    "            print(f\"MISSING FP16: alpha {a} seed {s}\")\n",
    "\n",
    "print(\"\\nScan Complete!\")\n",
    "print(f\"Found {sum(len(v) for v in int8_paths.values())} INT8 models.\")\n",
    "print(f\"Found {sum(len(v) for v in fp16_paths.values())} FP16 models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vO_wkjQ6izP"
   },
   "source": [
    "# Multi-Objective Evaluation & Weighted Selection\n",
    "\n",
    "**Objective:** To quantitatively evaluate all 27 model variations and identify the optimal candidate for satellite deployment using a mathematically grounded scoring system that balances **Accuracy**, **Latency**, and **Storage Size**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The Evaluation Framework\n",
    "To ensure statistical significance, we evaluate each of the three seeds for every combination of width multiplier ($\\alpha$) and precision format. Two distinct evaluation functions are utilized:\n",
    "\n",
    "* **`evaluate_fp32_latency`**: Benchmarks the original Keras `.h5` models.\n",
    "* **`evaluate_tflite`**: Benchmarks the optimized `.tflite` models. This function specifically handles **INT8 data scaling**, converting image tensors to 8-bit integers using the scale and zero-point metadata calculated during calibration.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Weighted Multi-Objective Scoring\n",
    "In orbital edge computing, a model that is 99% accurate but takes 2 seconds to run is as ineffective as a model that is 10% accurate but takes 1ms. We use **Min-Max Normalization** and a **Weighted Sum Model** to find the \"Pareto Optimal\" candidate.\n",
    "\n",
    "#### Step A: Normalization\n",
    "Because Accuracy ($0.0–1.0$), Latency (ms), and Size (MB) have different scales and units, we must normalize them to a range of $[0, 1]$.\n",
    "\n",
    "* **Accuracy ($A_n$):** Higher is better (Benefit criteria).\n",
    "    $$A_n = \\frac{Acc - Acc_{min}}{Acc_{max} - Acc_{min}}$$\n",
    "* **Latency ($L_n$) & Size ($S_n$):** Lower is better (Cost criteria). We invert these so that a value of $1.0$ represents the \"best\" (lowest) value.\n",
    "  $$L_n = \\frac{Lat_{max} - Lat}{Lat_{max} - Lat_{min}}$$\n",
    "\n",
    "  $$S_n = \\frac{Size_{max} - Size}{Size_{max} - Size_{min}}$$\n",
    "\n",
    "#### Step B: The Final Score Calculation\n",
    "We assign weights based on the deployment priority. For this satellite project, we prioritize **Efficiency** (Latency and Size) slightly over marginal accuracy gains:\n",
    "$$\\text{Score} = (0.30 \\times A_n) + (0.35 \\times L_n) + (0.35 \\times S_n)$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why This Approach?\n",
    "* **Engineering Rigor:** It moves the decision-making process from \"subjective guessing\" to \"data-driven selection.\"\n",
    "* **Deployment Constraints:** The 70% combined weight on Latency and Size reflects the reality of satellite hardware, where power and memory are strictly limited.\n",
    "* **Statistical Stability:** By calculating the **Mean** and **Standard Deviation** across three different seeds, we ensure that the \"Winner\" is a robust architecture rather than a result of fortunate weight initialization.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Result Interpretation Logic\n",
    "The script outputs two critical rankings:\n",
    "1.  **Averaged Winner:** The $\\alpha$ and precision variant that performs best on average across all seeds.\n",
    "2.  **Top Individual Asset:** The specific seed within that winning group that achieved the highest individual accuracy, designated as our **\"Flight-Ready\"** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6 — Evaluation with weighted multi-objective selection\n",
    "\n",
    "import time, os, pandas as pd\n",
    "\n",
    "def evaluate_tflite(model_path, dataset, max_samples=500):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    is_int8 = input_details['dtype'] == np.int8\n",
    "    scale, zero_point = input_details['quantization'] if is_int8 else (1.0, 0)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "    for images, labels in dataset.unbatch().take(max_samples):\n",
    "        img = images.numpy()\n",
    "        if is_int8:\n",
    "            img_q = np.round(img / scale + zero_point).astype(np.int8)\n",
    "            interpreter.set_tensor(input_details['index'], np.expand_dims(img_q, axis=0))\n",
    "        else:\n",
    "            interpreter.set_tensor(input_details['index'], np.expand_dims(img, axis=0))\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details['index'])\n",
    "        pred = np.argmax(output)\n",
    "        if pred == labels.numpy():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    end_time = time.time()\n",
    "    return (correct / total), ((end_time - start_time) / total * 1000)\n",
    "\n",
    "def evaluate_fp32_latency(model, dataset, max_samples=500):\n",
    "    correct, total = 0, 0\n",
    "    start_time = time.time()\n",
    "    for images, labels in dataset.unbatch().take(max_samples):\n",
    "        preds = model.predict(np.expand_dims(images.numpy(), axis=0), verbose=0)\n",
    "        pred = np.argmax(preds, axis=1)[0]\n",
    "        if pred == labels.numpy():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    end_time = time.time()\n",
    "    return (correct / total), ((end_time - start_time) / total * 1000)\n",
    "\n",
    "# Collect per-seed metrics\n",
    "per_seed = []\n",
    "for alpha in alphas:\n",
    "    for fp32_file, int8_file, fp16_file in zip(fp32_paths[alpha], int8_paths[alpha], fp16_paths[alpha]):\n",
    "        mdl = tf.keras.models.load_model(fp32_file)\n",
    "        fp32_acc, fp32_lat = evaluate_fp32_latency(mdl, test_ds)\n",
    "        int8_acc, int8_lat = evaluate_tflite(int8_file, test_ds)\n",
    "        fp16_acc, fp16_lat = evaluate_tflite(fp16_file, test_ds)\n",
    "        per_seed.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"seed\": fp32_file.split(\"seed\")[-1].split(\".\")[0],\n",
    "            \"fp32_acc\": fp32_acc, \"fp32_lat\": fp32_lat, \"fp32_size\": os.path.getsize(fp32_file)/1e6, \"fp32_path\": fp32_file,\n",
    "            \"int8_acc\": int8_acc, \"int8_lat\": int8_lat, \"int8_size\": os.path.getsize(int8_file)/1e6, \"int8_path\": int8_file,\n",
    "            \"fp16_acc\": fp16_acc, \"fp16_lat\": fp16_lat, \"fp16_size\": os.path.getsize(fp16_file)/1e6, \"fp16_path\": fp16_file,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(per_seed)\n",
    "\n",
    "def agg_stats(sub, prefix):\n",
    "    return {\n",
    "        \"mean_acc\": sub[f\"{prefix}_acc\"].mean(),\n",
    "        \"std_acc\": sub[f\"{prefix}_acc\"].std(),\n",
    "        \"mean_lat\": sub[f\"{prefix}_lat\"].mean(),\n",
    "        \"std_lat\": sub[f\"{prefix}_lat\"].std(),\n",
    "        \"mean_size\": sub[f\"{prefix}_size\"].mean(),\n",
    "        \"std_size\": sub[f\"{prefix}_size\"].std(),\n",
    "    }\n",
    "\n",
    "# Aggregate mean/std per alpha and variant\n",
    "agg = {}\n",
    "for alpha in alphas:\n",
    "    sub = df[df.alpha == alpha]\n",
    "    agg[alpha] = {\n",
    "        \"fp32\": agg_stats(sub, \"fp32\"),\n",
    "        \"fp16\": agg_stats(sub, \"fp16\"),\n",
    "        \"int8\": agg_stats(sub, \"int8\"),\n",
    "    }\n",
    "\n",
    "# Build a flat list of averaged models for scoring\n",
    "avg_models = []\n",
    "for alpha in agg:\n",
    "    for variant in [\"fp32\", \"fp16\", \"int8\"]:\n",
    "        m = agg[alpha][variant]\n",
    "        avg_models.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"variant\": variant,\n",
    "            \"mean_acc\": m[\"mean_acc\"],\n",
    "            \"mean_lat\": m[\"mean_lat\"],\n",
    "            \"mean_size\": m[\"mean_size\"],\n",
    "        })\n",
    "\n",
    "# Compute min/max for normalization\n",
    "acc_vals  = [m[\"mean_acc\"]  for m in avg_models]\n",
    "lat_vals  = [m[\"mean_lat\"]  for m in avg_models]\n",
    "size_vals = [m[\"mean_size\"] for m in avg_models]\n",
    "acc_min, acc_max = min(acc_vals), max(acc_vals)\n",
    "lat_min, lat_max = min(lat_vals), max(lat_vals)\n",
    "size_min, size_max = min(size_vals), max(size_vals)\n",
    "\n",
    "def norm_up(x, xmin, xmax):\n",
    "    return 0.0 if xmax == xmin else (x - xmin) / (xmax - xmin)\n",
    "\n",
    "def norm_down(x, xmin, xmax):\n",
    "    # higher is better, so invert for latency/size\n",
    "    return 0.0 if xmax == xmin else (xmax - x) / (xmax - xmin)\n",
    "\n",
    "# Weighted multi-objective score\n",
    "# Score = 0.30 * A_n + 0.35 * L_n + 0.35 * S_n\n",
    "for m in avg_models:\n",
    "    A_n = norm_up(m[\"mean_acc\"], acc_min, acc_max)\n",
    "    L_n = norm_down(m[\"mean_lat\"], lat_min, lat_max)\n",
    "    S_n = norm_down(m[\"mean_size\"], size_min, size_max)\n",
    "    m[\"score\"] = 0.30 * A_n + 0.35 * L_n + 0.35 * S_n\n",
    "    m[\"A_n\"] = A_n; m[\"L_n\"] = L_n; m[\"S_n\"] = S_n\n",
    "\n",
    "# Pick best averaged model by highest score\n",
    "best_avg = max(avg_models, key=lambda x: x[\"score\"])\n",
    "best_alpha, best_variant = best_avg[\"alpha\"], best_avg[\"variant\"]\n",
    "\n",
    "# Among that alpha+variant, pick best seed by highest accuracy (then lower latency)\n",
    "best_row = None\n",
    "for _, row in df[df.alpha == best_alpha].iterrows():\n",
    "    acc = row[f\"{best_variant}_acc\"]\n",
    "    lat = row[f\"{best_variant}_lat\"]\n",
    "    if (best_row is None) or (acc > best_row[\"acc\"]) or (acc == best_row[\"acc\"] and lat < best_row[\"lat\"]):\n",
    "        best_row = {\"acc\": acc, \"lat\": lat, \"row\": row}\n",
    "\n",
    "best_model_path = best_row[\"row\"][f\"{best_variant}_path\"]\n",
    "\n",
    "# Print mean/std and deltas vs FP32 of same alpha (speed, size, accuracy change)\n",
    "print(\"\\n=== Mean/Std by alpha & variant ===\")\n",
    "for alpha in alphas:\n",
    "    base = agg[alpha][\"fp32\"]\n",
    "    base_acc, base_lat, base_size = base[\"mean_acc\"], base[\"mean_lat\"], base[\"mean_size\"]\n",
    "    for variant in [\"fp32\",\"fp16\",\"int8\"]:\n",
    "        m = agg[alpha][variant]\n",
    "        acc = m[\"mean_acc\"]*100\n",
    "        lat = m[\"mean_lat\"]\n",
    "        size = m[\"mean_size\"]\n",
    "        std_acc = m[\"std_acc\"]*100\n",
    "        std_lat = m[\"std_lat\"]\n",
    "        std_size = m[\"std_size\"]\n",
    "        print(f\"alpha={alpha}, {variant}: Acc {acc:.2f}±{std_acc:.2f} %, \"\n",
    "              f\"Lat {lat:.2f}±{std_lat:.2f} ms, Size {size:.2f}±{std_size:.2f} MB\")\n",
    "        if variant != \"fp32\":\n",
    "            acc_delta = (m[\"mean_acc\"] - base_acc) * 100\n",
    "            speedup   = base_lat / lat if lat > 0 else float('inf')\n",
    "            size_red  = base_size / size if size > 0 else float('inf')\n",
    "            print(f\"    vs FP32 (same alpha): ΔAcc {acc_delta:.2f} pts, \"\n",
    "                  f\"Speedup {speedup:.2f}x, Size reduction {size_red:.2f}x\")\n",
    "\n",
    "print(\"\\n=== Weighted scores (A_n, L_n, S_n, Score) ===\")\n",
    "for m in sorted(avg_models, key=lambda x: -x[\"score\"]):\n",
    "    print(f\"alpha={m['alpha']}, {m['variant']}: \"\n",
    "          f\"A_n={m['A_n']:.3f}, L_n={m['L_n']:.3f}, S_n={m['S_n']:.3f}, \"\n",
    "          f\"Score={m['score']:.3f}\")\n",
    "\n",
    "print(f\"\\nBest averaged model by weighted score: alpha={best_alpha}, variant={best_variant}, \"\n",
    "      f\"Score={best_avg['score']:.3f}\")\n",
    "\n",
    "print(\"Best single model within that averaged winner (by seed, highest acc then lower lat):\")\n",
    "print(best_row[\"row\"])\n",
    "print(f\"\\nUse this for confusion matrix: {best_model_path}\")\n",
    "\n",
    "# (Comment: Score = 0.30 * A_n + 0.35 * L_n + 0.35 * S_n,\n",
    "#  where A_n = (Acc - Acc_min)/(Acc_max - Acc_min),\n",
    "#        L_n = (Lat_max - Lat)/(Lat_max - Lat_min),\n",
    "#        S_n = (Size_max - Size)/(Size_max - Size_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4Jv9MOG1pLa"
   },
   "source": [
    "# Section 7: Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7 — Plots (separate train/val, larger figs, 2D trade-off, orange CM)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Reload histories if not in memory, normalize keys to float\n",
    "if 'histories' not in globals():\n",
    "    with open(f\"{save_path}/histories.json\", \"r\") as f:\n",
    "        h = json.load(f)\n",
    "    histories = h\n",
    "histories = {float(k): v for k, v in histories.items()}\n",
    "\n",
    "# If alphas not in scope, derive from histories\n",
    "if 'alphas' not in globals():\n",
    "    alphas = sorted(histories.keys())\n",
    "\n",
    "# Consistent colors per alpha\n",
    "palette = {\n",
    "    alphas[0]: sns.color_palette(\"tab10\")[0],\n",
    "    alphas[1]: sns.color_palette(\"tab10\")[2],\n",
    "    alphas[2]: sns.color_palette(\"tab10\")[4],\n",
    "}\n",
    "\n",
    "def plot_mean_std_single(histories, alphas, metric='accuracy', title='Train Accuracy'):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for alpha in alphas:\n",
    "        accs = np.stack([h[metric] for h in histories[alpha]], axis=0)\n",
    "        mean_acc, std_acc = accs.mean(0), accs.std(0)\n",
    "        epochs_range = np.arange(1, len(mean_acc)+1)\n",
    "        c = palette[alpha]\n",
    "        plt.plot(epochs_range, mean_acc*100, label=f'alpha={alpha}', lw=2.5, color=c)\n",
    "        plt.fill_between(epochs_range, (mean_acc-std_acc)*100, (mean_acc+std_acc)*100,\n",
    "                         color=c, alpha=0.20)\n",
    "    plt.title(f'{title} Mean±Std over seeds', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=13); plt.ylabel('Accuracy (%)', fontsize=13)\n",
    "    plt.grid(True, alpha=0.3); plt.legend(fontsize=12)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_mean_std_train_val(histories, alphas):\n",
    "    # Train\n",
    "    plot_mean_std_single(histories, alphas, metric='accuracy', title='Training Accuracy')\n",
    "    # Val\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for alpha in alphas:\n",
    "        accs = np.stack([h['val_accuracy'] for h in histories[alpha]], axis=0)\n",
    "        mean_acc, std_acc = accs.mean(0), accs.std(0)\n",
    "        epochs_range = np.arange(1, len(mean_acc)+1)\n",
    "        c = palette[alpha]\n",
    "        plt.plot(epochs_range, mean_acc*100, label=f'alpha={alpha}', lw=2.5, color=c, linestyle='--')\n",
    "        plt.fill_between(epochs_range, (mean_acc-std_acc)*100, (mean_acc+std_acc)*100,\n",
    "                         color=c, alpha=0.20)\n",
    "    plt.title('Validation Accuracy Mean±Std over seeds', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=13); plt.ylabel('Accuracy (%)', fontsize=13)\n",
    "    plt.grid(True, alpha=0.3); plt.legend(fontsize=12)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_mean_std_train_val(histories, alphas)\n",
    "\n",
    "# 2D trade-off plot: Latency (x) vs Accuracy (y), marker size ~ model size, label bold\n",
    "def plot_tradeoffs_2d(agg):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    colors = {'fp32':'blue','fp16':'green','int8':'red'}\n",
    "    markers = {'fp32':'o','fp16':'^','int8':'s'}\n",
    "    for alpha in agg:\n",
    "        for variant in ['fp32','fp16','int8']:\n",
    "            m = agg[alpha][variant]\n",
    "            acc = m['mean_acc']*100\n",
    "            lat = m['mean_lat']\n",
    "            size = m['mean_size']\n",
    "            plt.scatter(lat, acc, s=max(40, size*60), color=colors[variant],\n",
    "                        marker=markers[variant], edgecolor='k', linewidth=0.8)\n",
    "            plt.text(lat, acc, f\"a{alpha}-{variant}\", fontsize=12, fontweight='bold',\n",
    "                     ha='left', va='bottom')\n",
    "    plt.xlabel('Latency (ms) ↓', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%) ↑', fontsize=14)\n",
    "    plt.title('Latency vs Accuracy (marker size ∝ model size)', fontsize=16, fontweight='bold')\n",
    "    # Custom legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elems = [\n",
    "        Line2D([0],[0], marker='o', color='w', label='fp32', markerfacecolor='blue', markersize=10, markeredgecolor='k'),\n",
    "        Line2D([0],[0], marker='^', color='w', label='fp16', markerfacecolor='green', markersize=10, markeredgecolor='k'),\n",
    "        Line2D([0],[0], marker='s', color='w', label='int8', markerfacecolor='red', markersize=10, markeredgecolor='k'),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elems, title='Variant', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_tradeoffs_2d(agg)\n",
    "\n",
    "# Confusion matrix for the best model (orange colormap)\n",
    "def plot_confusion_matrix_model(model_path, dataset, class_names, max_samples=300, variant='fp32'):\n",
    "    y_true, y_pred = [], []\n",
    "    if variant == 'fp32':\n",
    "        mdl = tf.keras.models.load_model(model_path)\n",
    "        for images, labels in dataset.unbatch().take(max_samples):\n",
    "            preds = mdl.predict(np.expand_dims(images.numpy(), axis=0), verbose=0)\n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(np.argmax(preds, axis=1)[0])\n",
    "    else:\n",
    "        interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        is_int8 = input_details['dtype'] == np.int8\n",
    "        scale, zero_point = input_details['quantization'] if is_int8 else (1.0, 0)\n",
    "        for images, labels in dataset.unbatch().take(max_samples):\n",
    "            img = images.numpy()\n",
    "            if is_int8:\n",
    "                img_q = np.round(img / scale + zero_point).astype(np.int8)\n",
    "                interpreter.set_tensor(input_details['index'], np.expand_dims(img_q, axis=0))\n",
    "            else:\n",
    "                interpreter.set_tensor(input_details['index'], np.expand_dims(img, axis=0))\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details['index'])\n",
    "            y_true.append(labels.numpy())\n",
    "            y_pred.append(np.argmax(output))\n",
    "\n",
    "    # Make cm global so Section 8 can use it\n",
    "    global cm\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix: {os.path.basename(model_path)}', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Class', fontsize=13); plt.xlabel('Predicted Class', fontsize=13)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_confusion_matrix_model(\n",
    "    best_model_path,\n",
    "    test_ds,\n",
    "    info.features['label'].names,\n",
    "    max_samples=300,\n",
    "    variant='fp32' if best_variant=='fp32' else 'tflite'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAfAniNYi4PM"
   },
   "source": [
    "# Section 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any true class with >3 total mistakes in cm (row off-diagonal), show:\n",
    "#   - 1 example image of the true class\n",
    "#   - 1 example image for each distinct predicted class that appeared in that row\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if 'cm' not in globals():\n",
    "    raise RuntimeError(\"Confusion matrix `cm` not found. Please run Section 7 first to populate `cm`.\")\n",
    "\n",
    "class_names = info.features['label'].names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "def get_one_image_by_label(dataset, label_id):\n",
    "    \"\"\"Return one preprocessed image (as np.array) from dataset matching label_id.\"\"\"\n",
    "    for img, lbl in dataset.unbatch():\n",
    "        if int(lbl.numpy()) == label_id:\n",
    "            return img.numpy()\n",
    "    return None\n",
    "\n",
    "for t in range(num_classes):\n",
    "    row = cm[t]\n",
    "    mistakes = row.sum() - row[t]\n",
    "    if mistakes > 3:\n",
    "        # predicted classes with nonzero confusions\n",
    "        pred_classes = [p for p in range(num_classes) if p != t and row[p] > 0]\n",
    "        if not pred_classes:\n",
    "            continue\n",
    "\n",
    "        # fetch one true image\n",
    "        true_img = get_one_image_by_label(test_ds, t)\n",
    "        if true_img is None:\n",
    "            continue\n",
    "\n",
    "        ncols = 1 + len(pred_classes)\n",
    "        plt.figure(figsize=(4 * ncols, 4))\n",
    "\n",
    "        # true image\n",
    "        plt.subplot(1, ncols, 1)\n",
    "        plt.imshow(np.clip((true_img + 1) / 2, 0, 1))\n",
    "        plt.title(f\"True: {class_names[t]}\", fontweight='bold')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # one example image per predicted class\n",
    "        for idx, p in enumerate(pred_classes, start=2):\n",
    "            pred_img = get_one_image_by_label(test_ds, p)\n",
    "            if pred_img is None:\n",
    "                continue\n",
    "            plt.subplot(1, ncols, idx)\n",
    "            plt.imshow(np.clip((pred_img + 1) / 2, 0, 1))\n",
    "            plt.title(f\"Pred class sample: {class_names[p]}\", color='red', fontweight='bold')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(\n",
    "            f\"Mistakes for true '{class_names[t]}' (off-diagonal count={int(mistakes)})\",\n",
    "            fontsize=14, fontweight='bold'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
